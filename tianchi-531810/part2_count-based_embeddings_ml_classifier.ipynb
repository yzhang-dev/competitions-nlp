{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Count-based Embeddings + ML Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_to_scv(pred, \n",
    "                  sample_csv='data/test_a_sample_submit.csv', \n",
    "                  path='submissions/sub.csv'):\n",
    "    \n",
    "    sub = pd.read_csv(sample_csv, index_col=False)\n",
    "    sub['label'] = pred\n",
    "    sub.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.44 s, sys: 954 ms, total: 8.39 s\n",
      "Wall time: 8.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df = pd.read_csv('data/train_set.csv', sep='\\t', index_col=False)\n",
    "test_df = pd.read_csv('data/test_a.csv', sep='\\t', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, test_text = train_df['text'], test_df['text']\n",
    "corpus = pd.concat([train_text, test_text], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "A text is reduced to the **base vocabulary** (after *tokenization*, *stopword removal*, *stemming*, *lemmatization*, *indexing*) it uses, a **representation**.\n",
    "\n",
    "* [One-hot](http://localhost:8888/notebooks/tianchi-531810/part2_TF-IDF_ml.ipynb#One-hot)\n",
    "* [N-gram](http://localhost:8888/notebooks/tianchi-531810/part2_TF-IDF_ml.ipynb#N-gram)\n",
    "* [Bag-of-Words (BoW)](http://localhost:8888/notebooks/tianchi-531810/part2_TF-IDF_ml.ipynb#Bag-of-Words-(BoW))\n",
    "* [TF-IDF](http://localhost:8888/notebooks/tianchi-531810/part2_TF-IDF_ml.ipynb#TF-IDF)\n",
    "* [Latent Semantic Analysis (LSA)](http://localhost:8888/notebooks/tianchi-531810/part2_TF-IDF_ml.ipynb#Latent-Semantic-Analysis-(LSA))\n",
    "* [Word2Vec](https://en.wikipedia.org/wiki/Word2vec)\n",
    "* [GloVe](https://en.wikipedia.org/wiki/GloVe_(machine_learning))\n",
    "* [fastText](https://en.wikipedia.org/wiki/FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = None\n",
    "#STOP_WORDS = ['3750', '648', '900']\n",
    "NGRAM_RANGE=(1, 1)  # only unigram / BoW\n",
    "#NGRAM_RANGE=(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [One-hot](https://en.wikipedia.org/wiki/One-hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [N-gram](https://en.wikipedia.org/wiki/N-gram)\n",
    "\n",
    "We can view Bag-of-Words as a special case of the $n$-gram, with $n=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Bag-of-Words (BoW)](https://en.wikipedia.org/wiki/Bag-of-words_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 4s, sys: 3.89 s, total: 4min 7s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    stop_words=STOP_WORDS,\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=NGRAM_RANGE\n",
    ")\n",
    "\n",
    "count_vectorizer.fit(corpus)  # avoid OOV\n",
    "train_count_features = count_vectorizer.transform(train_text)  # .shape: (N_train, V)\n",
    "test_count_features = count_vectorizer.transform(test_text)    # .shape: (N_test, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "\n",
    "Term frequencyâ€“inverse document frequency (TF-IDF), one of the most popular term-weighting schemes today, gives a term-based sparse representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 4s, sys: 4.01 s, total: 4min 8s\n",
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words=STOP_WORDS,\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=NGRAM_RANGE\n",
    ")\n",
    "\n",
    "tfidf_vectorizer.fit(corpus)  # avoid OOV\n",
    "train_tfidf_features = tfidf_vectorizer.transform(train_text)  # .shape: (N_train, V)\n",
    "test_tfidf_features = tfidf_vectorizer.transform(test_text)    # .shape: (N_test, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis) (LSA)\n",
    "\n",
    "Term-based representation is reduced to a concept-based one through Singular Value Decomposition (SVD, a \"non-square\" PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "\n",
    "### Logistic regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8453524000909389\n",
      "CPU times: user 12.2 s, sys: 15.2 s, total: 27.3 s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "lr_clf = LogisticRegression(\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X, y = train_tfidf_features.toarray(), train_df['label'].to_numpy()\n",
    "X_test = test_tfidf_features.toarray()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "lr_clf.fit(X_train[:10000], y_train[:10000])\n",
    "y_pred_lr = lr_clf.predict(X_val)\n",
    "score = f1_score(y_val, y_pred_lr, average='macro')\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8609733200311974\n",
      "CPU times: user 22.5 s, sys: 1.28 s, total: 23.8 s\n",
      "Wall time: 8.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "ridge_clf = RidgeClassifier(\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ridge_clf.fit(X_train[:10000], y_train[:10000])\n",
    "y_pred_ridge = ridge_clf.predict(X_val)\n",
    "score = f1_score(y_val, y_pred_ridge, average='macro')\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8717562197605281\n",
      "CPU times: user 28.4 s, sys: 84 ms, total: 28.5 s\n",
      "Wall time: 4.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_clf = SGDClassifier(\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_clf.fit(X_train[:10000], y_train[:10000])\n",
    "y_pred_svm = svm_clf.predict(X_val)\n",
    "score = f1_score(y_val, y_pred_svm, average='macro')\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
